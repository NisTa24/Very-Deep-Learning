{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Tx1wpXQYkNM5",
    "outputId": "d1f9bd40-c4f9-4cb0-e217-c9942763341b"
   },
   "outputs": [],
   "source": [
    "! pip install torchvision\n",
    "! pip install torchfcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "qzsScweurktu",
    "outputId": "3e8e258d-2e75-4cd3-9d36-8dac7742598a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root = \".\"\n",
    "!ls .\n",
    "os.chdir(root)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XirR3P-_IYKh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import shutil\n",
    "import collections\n",
    "import PIL\n",
    "import torchfcn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import Counter\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(60),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbTsi_OOIc3r"
   },
   "outputs": [],
   "source": [
    "# create dataset class\n",
    "class Bags(Dataset):\n",
    "  \"\"\"Bags dataset\"\"\"\n",
    "  class_names = np.array([\n",
    "        'background',\n",
    "        'bag'\n",
    "    ])\n",
    "  mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n",
    "\n",
    "  def __init__(self, root, split='train', transform=False):\n",
    "      self.root = root\n",
    "      self.split = split\n",
    "      self._transform = transform\n",
    "\n",
    "      dataset_dir = osp.join(self.root, 'bags_data')\n",
    "      self.files = collections.defaultdict(list)\n",
    "      for split in ['train', 'val']:\n",
    "          imgsets_file = osp.join(\n",
    "              dataset_dir, 'imagesets/%s.txt' % split)\n",
    "          for did in open(imgsets_file):\n",
    "              did = did.strip()\n",
    "              img_file = osp.join(dataset_dir, 'JPEGImages/%s.jpg' % did)\n",
    "              lbl_file = osp.join(\n",
    "                  dataset_dir, 'segmentation_mask/%s.png' % did)\n",
    "              self.files[split].append({\n",
    "                  'img': img_file,\n",
    "                  'lbl': lbl_file,\n",
    "              })\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.files[self.split])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      data_file = self.files[self.split][index]\n",
    "      # load image\n",
    "      img_file = data_file['img']\n",
    "      img = PIL.Image.open(img_file).convert('RGB')\n",
    "      img = img.resize((224, 224))\n",
    "      if img.getbands()[0] == 'L':\n",
    "        img = img.convert('RGB')\n",
    "      img = np.array(img, dtype=np.uint8)\n",
    "      # load label\n",
    "      lbl_file = data_file['lbl']\n",
    "      lbl = PIL.Image.open(lbl_file)\n",
    "      lbl = lbl.resize((224, 224))\n",
    "      lbl = np.array(lbl, dtype=np.int32)\n",
    "      lbl[lbl == 255] = -1\n",
    "      if self._transform:\n",
    "        return self.transform(img, lbl)\n",
    "      else:\n",
    "        return img, lbl\n",
    "\n",
    "  def transform(self, img, lbl):\n",
    "      img = img.astype(np.float64)\n",
    "      img -= self.mean_bgr\n",
    "      img = img.transpose(2, 0, 1)\n",
    "      img = torch.from_numpy(img).float()\n",
    "      lbl = torch.from_numpy(lbl).long()\n",
    "      return img, lbl\n",
    "\n",
    "  def untransform(self, img, lbl):\n",
    "      img = img.numpy()\n",
    "      img = img.transpose(1, 2, 0)\n",
    "      img += self.mean_bgr\n",
    "      img = img.astype(np.uint8)\n",
    "      img = img[:, :, ::-1]\n",
    "      lbl = lbl.numpy()\n",
    "      return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XArRa_Zr9vHV"
   },
   "outputs": [],
   "source": [
    "# create data sets and dataloaders\n",
    "train_set = Bags(root, \"train\", True)\n",
    "test_set = Bags(root, \"val\", True)\n",
    "\n",
    "#use cuda\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n",
    "train_loader = DataLoader(train_set, batch_size = 32, shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(test_set, batch_size = 32, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "cfjlK-vPKKHC",
    "outputId": "0fda7569-9d6e-4fb8-8f21-0a8cc7e09b92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_parameters(model, bias=False):\n",
    "    import torch.nn as nn\n",
    "    modules_skipped = (\n",
    "        nn.ReLU,\n",
    "        nn.MaxPool2d,\n",
    "        nn.Dropout2d,\n",
    "        nn.Sequential,\n",
    "        torchfcn.models.FCN32s,\n",
    "        torchfcn.models.FCN16s,\n",
    "        torchfcn.models.FCN8s,\n",
    "    )\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if bias:\n",
    "                yield m.bias\n",
    "            else:\n",
    "                yield m.weight\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            # weight is frozen because it is just a bilinear upsampling\n",
    "            if bias:\n",
    "                assert m.bias is None\n",
    "        elif isinstance(m, modules_skipped):\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError('Unexpected module: %s' % str(m))\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#checkpoint path file\n",
    "resume =  None\n",
    "\n",
    "# Create model\n",
    "model = torchfcn.models.FCN32s(n_class=2)\n",
    "start_epoch = 0\n",
    "start_iteration = 0\n",
    "\n",
    "# use a pretrained .pth file if provided.\n",
    "if resume:\n",
    "    checkpoint = torch.load(resume)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_iteration = checkpoint['iteration']\n",
    "else:\n",
    "    vgg16 = torchfcn.models.VGG16(pretrained=True)\n",
    "    model.copy_params_from_vgg16(vgg16)\n",
    "\n",
    "# use cuda option\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    [\n",
    "        {'params': get_parameters(model, bias=False)},\n",
    "        {'params': get_parameters(model, bias=True),\n",
    "         'lr': lr * 2, 'weight_decay': 0},\n",
    "    ],\n",
    "    lr=lr)\n",
    "\n",
    "if resume:\n",
    "    optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "\n",
    "# create a trainer object\n",
    "trainer = torchfcn.Trainer(\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    out=\".\",\n",
    "    max_iter=1000,\n",
    "    interval_validate=400,\n",
    ")\n",
    "trainer.epoch = start_epoch\n",
    "trainer.iteration = start_iteration\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def validate():\n",
    "    visualizations = []\n",
    "    label_trues, label_preds = [], []\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(val_loader),\n",
    "                                               total=len(val_loader),\n",
    "                                               ncols=80, leave=False):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        score = model(data)\n",
    "\n",
    "        imgs = data.data.cpu()\n",
    "        lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "        lbl_true = target.data.cpu()\n",
    "        for img, lt, lp in zip(imgs, lbl_true, lbl_pred):\n",
    "            img, lt = val_loader.dataset.untransform(img, lt)\n",
    "            label_trues.append(lt)\n",
    "            label_preds.append(lp)\n",
    "            if len(visualizations) < 9:\n",
    "                viz = fcn.utils.visualize_segmentation(\n",
    "                    lbl_pred=lp, lbl_true=lt, img=img, n_class=n_class,\n",
    "                    label_names=val_loader.dataset.class_names)\n",
    "                visualizations.append(viz)\n",
    "    metrics = torchfcn.utils.label_accuracy_score(\n",
    "        label_trues, label_preds, n_class=n_class)\n",
    "    metrics = np.array(metrics)\n",
    "    metrics *= 100\n",
    "    print('''\\Accuracy: {0}, Accuracy Class: {1}, Mean IU: {2}, FWAV Accuracy: {3}'''.format(*metrics))\n",
    "    viz = fcn.utils.get_tile_image(visualizations)\n",
    "    skimage.io.imsave('viz_evaluate.png', viz)\n",
    "\n",
    "validate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Exercise3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8xF-M4CZLwM"
   },
   "source": [
    "# Exercise Sheet 1\n",
    "\n",
    "\n",
    "\n",
    "### Read the Dataset\n",
    "\n",
    "- Use Pandas to read the 'covertype.csv' file\n",
    "- The dataset contains information on different forest cover types\n",
    "- Look at the columns. Which of them contain meaningful features?\n",
    "\n",
    "\n",
    "\n",
    "### Seperate Features and Labels\n",
    "- Define x as the vectors of meaningful features\n",
    "- Define y as the labels (Cover_Type)\n",
    "\n",
    "\n",
    "\n",
    "### Split the dataset into two disjoint datasets for training and testing\n",
    "- Randomly split the dataset. Use 70% for training and 30% for testing.\n",
    "- Define x_train and x_test as the feature vectors\n",
    "- Define y_train and y_test as the labels\n",
    "    - Hint: Have a look at the sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOsbsz_LZLwO",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2596,   51,    3, ...,    0,    0,    0],\n",
       "       [2590,   56,    2, ...,    0,    0,    0],\n",
       "       [2804,  139,    9, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [2492,  134,   25, ...,    0,    0,    0],\n",
       "       [2487,  167,   28, ...,    0,    0,    0],\n",
       "       [2475,  197,   34, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"covertype.csv\")\n",
    "x = np.asarray(data.iloc[:, 1:-1])\n",
    "# Adjust the labels between [0..n-1]\n",
    "y = np.asarray([x-1 for x in data.iloc[:, -1]])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.70,\n",
    "                                            test_size = 0.30, random_state=42)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IphJtf3ZZLwS"
   },
   "source": [
    "### Train a simple deep neural network\n",
    "- Use Pytorch to define a simple Multi-Layer Perceptron with at least 3 layers\n",
    "    - The input layer should have as many neurons as there are features\n",
    "        - How many features are there?\n",
    "    - The last layer should have as many neurons as there are classes\n",
    "        - How many classes are there?\n",
    "- Pack your training and testing datasets in a class which inherits from torch.utils.data.Datset\n",
    "    - features you input to your network should be of type torch.float\n",
    "    - labels should be of type torch.long.\n",
    "- Use a torch.utils.data.DataLoader to access your data in batches\n",
    "- Train the MLP with your data from the train_loader using Cross-Entropy Loss and the Adam Optimizer\n",
    "    - Make sure to save the training history for later assessment\n",
    "- Evaluate the performance on your test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uknk7pFZZLwU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# specify the dtype of input layer and output layer\n",
    "dtype_in = torch.float\n",
    "dtype_out = torch.long\n",
    "\n",
    "# Define your MLP\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input, h1, h2, n_output):\n",
    "      super(Model, self).__init__()\n",
    "      self.layer1 = nn.Linear(n_input, h1)\n",
    "      self.layer2 = nn.Linear(h1, h2)\n",
    "      self.layer3 = nn.Linear(h2, n_output)\n",
    "\n",
    "    def forward(self, input):\n",
    "      h1_relu = F.relu(self.layer1(input))\n",
    "      h2_relu = F.relu(self.layer2(h1_relu))\n",
    "      y_pred = F.softmax(self.layer3(h2_relu))\n",
    "      return y_pred\n",
    "\n",
    "class ForestCoverDataset(Dataset):\n",
    "    \"\"\"Forest Cover dataset.\"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "      self.features = torch.tensor(features, dtype = dtype_in)\n",
    "      self.labels = torch.tensor(labels, dtype = dtype_out)\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      feature = self.features[idx]\n",
    "      label = self.labels[idx]\n",
    "\n",
    "      return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1171,
     "status": "ok",
     "timestamp": 1573127788610,
     "user": {
      "displayName": "Karthik Kr",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBCoqLlSYB2Yij1yRn8pM9cf38YifRpaIU1zBe7sw=s64",
      "userId": "09696644089968782207"
     },
     "user_tz": -60
    },
    "id": "p2FqdE1wZLwW",
    "outputId": "40260ab1-3b63-4ac1-842e-7e01cf2c779d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimention : 54\n",
      "Output Dimention : 7\n",
      "Batch Size : 64\n",
      "Learning Rate : 0.1\n"
     ]
    }
   ],
   "source": [
    "train_set = ForestCoverDataset(x_train, y_train)\n",
    "test_set = ForestCoverDataset(x_test, y_test)\n",
    "\n",
    "# set parameters\n",
    "# input dimention\n",
    "in_dim = len(x_train[0])\n",
    "\n",
    "# output dimention\n",
    "out_dim = len(np.unique(y_train))\n",
    "\n",
    "# batch size\n",
    "BS = 64\n",
    "\n",
    "# define number of neurons in each layer\n",
    "n_input, h1, h2, n_out = in_dim, 120, 100, out_dim\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "print(\"Input Dimention : {}\".format(in_dim))\n",
    "print(\"Output Dimention : {}\".format(out_dim))\n",
    "print(\"Batch Size : {}\".format(BS))\n",
    "print(\"Learning Rate : {}\".format(lr))\n",
    "\n",
    "# Create your DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size = BS, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size = BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1573127788610,
     "user": {
      "displayName": "Karthik Kr",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBCoqLlSYB2Yij1yRn8pM9cf38YifRpaIU1zBe7sw=s64",
      "userId": "09696644089968782207"
     },
     "user_tz": -60
    },
    "id": "msrVwrA8ZLwZ",
    "outputId": "0a84f624-079b-4b0e-abd8-7e54fbe9d380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Linear(in_features=54, out_features=120, bias=True)\n",
      "  (layer2): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=7, bias=True)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the network, the optimizer and the Loss Criterion\n",
    "model = Model(n_input, h1, h2, n_out)\n",
    "print(model)\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(criterion)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6w0ortDZLwb"
   },
   "outputs": [],
   "source": [
    "# Define your training Loop\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model = model.float()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0.\n",
    "    samples = 0.\n",
    "    for sample in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data, label = sample\n",
    "        prediction = model(data)\n",
    "        loss = criterion(prediction, label)\n",
    "        correct += sum(torch.argmax(prediction, dim=1) == label)\n",
    "        samples += len(data)\n",
    "        epoch_loss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return epoch_loss, correct/samples\n",
    "\n",
    "\n",
    "# Define your testing Loop\n",
    "def test_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0.\n",
    "    samples = 0.\n",
    "    for sample in dataloader:\n",
    "        data, label = sample\n",
    "        prediction = model(data)\n",
    "        loss = criterion(prediction, label)\n",
    "        correct += sum(torch.argmax(prediction, dim=1) == label)\n",
    "        samples += len(data)\n",
    "        epoch_loss += loss.data\n",
    "    return epoch_loss, correct/samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1573127788813,
     "user": {
      "displayName": "Karthik Kr",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBCoqLlSYB2Yij1yRn8pM9cf38YifRpaIU1zBe7sw=s64",
      "userId": "09696644089968782207"
     },
     "user_tz": -60
    },
    "id": "qe7cDR57ZLwf",
    "outputId": "0bd6a218-f966-4ea7-8b3a-914976cac971"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(144.0328), tensor(0, dtype=torch.uint8))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if your training and testing loops are working\n",
    "train_epoch(model, train_loader, criterion, optimizer)\n",
    "test_epoch(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yadFBSZ2ZLwi"
   },
   "source": [
    "### Debug\n",
    "- If there is a RuntimeError raised in you loss function, either your network architecture or your data is faulty\n",
    "    - Check your network architecture\n",
    "    - Check your data\n",
    "        - Are there any NaN or infinite features or labels?\n",
    "    - Print the labels.\n",
    "        - How many unique labels do you have?\n",
    "        - Are they [0, ..., n-1]?\n",
    "            - If not, align them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLd8xyzoZLwj"
   },
   "outputs": [],
   "source": [
    "# Inspect your data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzO_-iEwZLwm"
   },
   "outputs": [],
   "source": [
    "# Fix your data and recreate your DataLoaders\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjprZnr-ZLwn"
   },
   "source": [
    "### Train your Network\n",
    "- Reinitialize your MLP from above and train it for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62607,
     "status": "ok",
     "timestamp": 1573120417795,
     "user": {
      "displayName": "Karthik Kr",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBCoqLlSYB2Yij1yRn8pM9cf38YifRpaIU1zBe7sw=s64",
      "userId": "09696644089968782207"
     },
     "user_tz": -60
    },
    "id": "zUToC9dYZLwo",
    "outputId": "70e82c5d-c703-4f29-e315-cb6272de5745"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa8243bbde3472f81a6b0703093cfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# net = \n",
    "# optimizer = \n",
    "train_loss, train_accuracy = [], []\n",
    "test_loss, test_accuracy = [], []\n",
    "\n",
    "for i in tqdm.tnrange(100):\n",
    "    loss, accuracy = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    train_loss.append(loss)\n",
    "    train_accuracy.append(accuracy)\n",
    "    loss, accuracy = test_epoch(model, test_loader, criterion)\n",
    "    test_loss.append(loss)\n",
    "    test_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9gnGVvCZLwq"
   },
   "source": [
    "### Does it work?\n",
    "- There should not be a RuntimeError raised now\n",
    "- Does the network converge / Does the loss decrease?\n",
    "\n",
    "\n",
    "### Visualize the training\n",
    "- use matplotlib.pyplot to visualize the history\n",
    "- plot both the training accuracy and the validation accuracy\n",
    "- Does the training stagnate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTOXnYBeZLwr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"accuracy\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_accuracy, train_loss, label = \"train set\")\n",
    "plt.plot(test_accuracy, test_loss, label = \"test set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9nGp0iMZLwt"
   },
   "source": [
    "### Inspect the data\n",
    "- Compute the min, max, mean and standard deviation of each feature\n",
    "- What data type do the columns have?\n",
    "- Use Pandas to print the statistics in a table\n",
    "- What could be problematic with the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56Z-Sp8NZLwt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int64</td>\n",
       "      <td>1888</td>\n",
       "      <td>3849</td>\n",
       "      <td>2752.25</td>\n",
       "      <td>416.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>157.22</td>\n",
       "      <td>110.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>16.43</td>\n",
       "      <td>8.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1343</td>\n",
       "      <td>227.55</td>\n",
       "      <td>209.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>int64</td>\n",
       "      <td>-134</td>\n",
       "      <td>547</td>\n",
       "      <td>51.26</td>\n",
       "      <td>60.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6836</td>\n",
       "      <td>1722.01</td>\n",
       "      <td>1334.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>212.45</td>\n",
       "      <td>30.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>int64</td>\n",
       "      <td>99</td>\n",
       "      <td>254</td>\n",
       "      <td>219.01</td>\n",
       "      <td>22.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>135.51</td>\n",
       "      <td>45.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6853</td>\n",
       "      <td>1516.27</td>\n",
       "      <td>1101.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type   Min   Max    Mean     Std\n",
       "0   int64  1888  3849 2752.25  416.07\n",
       "1   int64     0   360  157.22  110.71\n",
       "2   int64     0    52   16.43    8.42\n",
       "3   int64     0  1343  227.55  209.00\n",
       "4   int64  -134   547   51.26   60.98\n",
       "5   int64     0  6836 1722.01 1334.71\n",
       "6   int64     0   254  212.45   30.57\n",
       "7   int64    99   254  219.01   22.71\n",
       "8   int64     0   248  135.51   45.77\n",
       "9   int64     0  6853 1516.27 1101.83\n",
       "10  int64     0     1    0.24    0.43\n",
       "11  int64     0     1    0.03    0.18\n",
       "12  int64     0     1    0.42    0.49\n",
       "13  int64     0     1    0.31    0.46\n",
       "14  int64     0     1    0.02    0.15\n",
       "15  int64     0     1    0.04    0.20\n",
       "16  int64     0     1    0.06    0.24\n",
       "17  int64     0     1    0.05    0.23\n",
       "18  int64     0     1    0.01    0.10\n",
       "19  int64     0     1    0.04    0.20\n",
       "20  int64     0     0    0.00    0.00\n",
       "21  int64     0     1    0.00    0.01\n",
       "22  int64     0     1    0.00    0.03\n",
       "23  int64     0     1    0.14    0.35\n",
       "24  int64     0     1    0.03    0.16\n",
       "25  int64     0     1    0.02    0.12\n",
       "26  int64     0     1    0.03    0.17\n",
       "27  int64     0     1    0.01    0.10\n",
       "28  int64     0     0    0.00    0.00\n",
       "29  int64     0     1    0.01    0.08\n",
       "30  int64     0     1    0.04    0.19\n",
       "31  int64     0     1    0.00    0.06\n",
       "32  int64     0     1    0.00    0.05\n",
       "33  int64     0     1    0.01    0.10\n",
       "34  int64     0     1    0.00    0.04\n",
       "35  int64     0     1    0.02    0.15\n",
       "36  int64     0     1    0.05    0.22\n",
       "37  int64     0     1    0.02    0.13\n",
       "38  int64     0     1    0.00    0.01\n",
       "39  int64     0     1    0.00    0.06\n",
       "40  int64     0     1    0.00    0.04\n",
       "41  int64     0     1    0.00    0.03\n",
       "42  int64     0     1    0.09    0.28\n",
       "43  int64     0     1    0.05    0.21\n",
       "44  int64     0     1    0.02    0.15\n",
       "45  int64     0     1    0.05    0.21\n",
       "46  int64     0     1    0.04    0.20\n",
       "47  int64     0     1    0.00    0.04\n",
       "48  int64     0     1    0.01    0.08\n",
       "49  int64     0     1    0.00    0.02\n",
       "50  int64     0     1    0.00    0.05\n",
       "51  int64     0     1    0.05    0.21\n",
       "52  int64     0     1    0.04    0.20\n",
       "53  int64     0     1    0.03    0.17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "stats = pd.DataFrame(columns=[\"Type\", \"Min\", \"Max\", \"Mean\", \"Std\"])\n",
    "\n",
    "# Compute the values for each column\n",
    "typ = []\n",
    "for i in range(in_dim):\n",
    "  typ.append(x_train[:, i].dtype)\n",
    "cmin, cmax, cmean, cstd = np.amin(x_train, axis = 0),\\\n",
    "                          np.amax(x_train, axis = 0),\\\n",
    "                          np.mean(x_train, axis = 0),\\\n",
    "                          np.std(x_train, axis = 0)\n",
    "\n",
    "stats[\"Type\"], stats[\"Min\"], stats[\"Max\"], stats[\"Mean\"], stats[\"Std\"]\\\n",
    "= typ, cmin, cmax, cmean, cstd\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8VYx3w1ZLwu"
   },
   "source": [
    "### Preprocess the Data\n",
    "- Normalize or standardize your data, so all features are at the same scale.\n",
    "    - This will help your network to use all available features and not be biased by some features with large values\n",
    "    - Does it make sense to normalize all columns, or only some?\n",
    "- Hint: Again, look if you find something useful in sklearn\n",
    "\n",
    "\n",
    "- Never use test data to optimize your training! This includes the preprocessing\n",
    "    - Find preprocessing parameters on your training data only!\n",
    "    - Transform all your data with the computed parameters\n",
    "    - You have to remember which of your samples are used for training and which are for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2raF84rZLwv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#Normalize the data\n",
    "x_train = preprocessing.normalize(x_train, \"l2\")\n",
    "x_test = preprocessing.normalize(x_test, \"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXMUDcUiZLww"
   },
   "source": [
    "### Inspect data again\n",
    "- Print the statistics of the preprocessed data using the code from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y0zz4VYZLww"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type   Min  Max  Mean  Std\n",
       "0   float64  0.32 0.99  0.77 0.14\n",
       "1   float64  0.00 0.17  0.05 0.04\n",
       "2   float64  0.00 0.02  0.01 0.00\n",
       "3   float64  0.00 0.28  0.06 0.05\n",
       "4   float64 -0.04 0.14  0.01 0.02\n",
       "5   float64  0.00 0.89  0.41 0.20\n",
       "6   float64  0.00 0.13  0.06 0.02\n",
       "7   float64  0.02 0.12  0.06 0.02\n",
       "8   float64  0.00 0.11  0.04 0.02\n",
       "9   float64  0.00 0.92  0.37 0.17\n",
       "10  float64  0.00 0.00  0.00 0.00\n",
       "11  float64  0.00 0.00  0.00 0.00\n",
       "12  float64  0.00 0.00  0.00 0.00\n",
       "13  float64  0.00 0.00  0.00 0.00\n",
       "14  float64  0.00 0.00  0.00 0.00\n",
       "15  float64  0.00 0.00  0.00 0.00\n",
       "16  float64  0.00 0.00  0.00 0.00\n",
       "17  float64  0.00 0.00  0.00 0.00\n",
       "18  float64  0.00 0.00  0.00 0.00\n",
       "19  float64  0.00 0.00  0.00 0.00\n",
       "20  float64  0.00 0.00  0.00 0.00\n",
       "21  float64  0.00 0.00  0.00 0.00\n",
       "22  float64  0.00 0.00  0.00 0.00\n",
       "23  float64  0.00 0.00  0.00 0.00\n",
       "24  float64  0.00 0.00  0.00 0.00\n",
       "25  float64  0.00 0.00  0.00 0.00\n",
       "26  float64  0.00 0.00  0.00 0.00\n",
       "27  float64  0.00 0.00  0.00 0.00\n",
       "28  float64  0.00 0.00  0.00 0.00\n",
       "29  float64  0.00 0.00  0.00 0.00\n",
       "30  float64  0.00 0.00  0.00 0.00\n",
       "31  float64  0.00 0.00  0.00 0.00\n",
       "32  float64  0.00 0.00  0.00 0.00\n",
       "33  float64  0.00 0.00  0.00 0.00\n",
       "34  float64  0.00 0.00  0.00 0.00\n",
       "35  float64  0.00 0.00  0.00 0.00\n",
       "36  float64  0.00 0.00  0.00 0.00\n",
       "37  float64  0.00 0.00  0.00 0.00\n",
       "38  float64  0.00 0.00  0.00 0.00\n",
       "39  float64  0.00 0.00  0.00 0.00\n",
       "40  float64  0.00 0.00  0.00 0.00\n",
       "41  float64  0.00 0.00  0.00 0.00\n",
       "42  float64  0.00 0.00  0.00 0.00\n",
       "43  float64  0.00 0.00  0.00 0.00\n",
       "44  float64  0.00 0.00  0.00 0.00\n",
       "45  float64  0.00 0.00  0.00 0.00\n",
       "46  float64  0.00 0.00  0.00 0.00\n",
       "47  float64  0.00 0.00  0.00 0.00\n",
       "48  float64  0.00 0.00  0.00 0.00\n",
       "49  float64  0.00 0.00  0.00 0.00\n",
       "50  float64  0.00 0.00  0.00 0.00\n",
       "51  float64  0.00 0.00  0.00 0.00\n",
       "52  float64  0.00 0.00  0.00 0.00\n",
       "53  float64  0.00 0.00  0.00 0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = pd.DataFrame(columns=[\"Type\", \"Min\", \"Max\", \"Mean\", \"Std\"])\n",
    "\n",
    "# Compute the values for each column\n",
    "typ = []\n",
    "for i in range(in_dim):\n",
    "  typ.append(x_train[:, i].dtype)\n",
    "cmin, cmax, cmean, cstd = np.amin(x_train, axis = 0),\\\n",
    "                          np.amax(x_train, axis = 0),\\\n",
    "                          np.mean(x_train, axis = 0),\\\n",
    "                          np.std(x_train, axis = 0)\n",
    "\n",
    "stats[\"Type\"], stats[\"Min\"], stats[\"Max\"], stats[\"Mean\"], stats[\"Std\"]\\\n",
    "= typ, cmin, cmax, cmean, cstd\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BFs4gEyZLwx"
   },
   "source": [
    "### Train the network again\n",
    "- Recreate your DataLoaders with the normalized data\n",
    "- Reinitialize or your MLP from above and train it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZV1qyh0ZLwx"
   },
   "outputs": [],
   "source": [
    "# Recreate your DataLoaders with the normalized data\n",
    "train_set = ForestCoverDataset(x_train, y_train)\n",
    "test_set = ForestCoverDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = BS, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = BS, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9aePmdpZLwz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3843aaf001654de6b1261a21d3dfe44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "model = Model(n_input, h1, h2, n_out)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_loss, train_accuracy = [], []\n",
    "test_loss, test_accuracy = [], []\n",
    "\n",
    "for i in tqdm.tnrange(100):\n",
    "    loss, accuracy = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    train_loss.append(loss)\n",
    "    train_accuracy.append(accuracy)\n",
    "    loss, accuracy = test_epoch(model, train_loader, criterion)\n",
    "    test_loss.append(loss)\n",
    "    test_accuracy.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWs4-M0HZLw1"
   },
   "source": [
    "### Visualize the training\n",
    "- use matplotlib.pyplot to visualize the history\n",
    "- plot both the training accuracy and the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3Po-4MCZLw1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"accuracy\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_accuracy, train_loss, label = \"train set\")\n",
    "plt.plot(test_accuracy, test_loss, label = \"test set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "exercise_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
